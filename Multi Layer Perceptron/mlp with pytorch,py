import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim

# Step 1: Generate a synthetic dataset
X, y = make_classification(
    n_samples=1000,    # Total number of samples
    n_features=2,      # Number of features (2 for visualization purposes)
    n_informative=2,   # Number of informative features
    n_redundant=0,     # Number of redundant features
    random_state=42    # Seed for reproducibility
)

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,     # 20% of the data for testing
    random_state=42    # Seed for reproducibility
)

# Step 3: Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert the data to PyTorch tensors
X_train_tensor = torch.FloatTensor(X_train)
X_test_tensor = torch.FloatTensor(X_test)
y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)  # Add an extra dimension for binary classification
y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)

# Step 4: Define the MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(2, 32)  # Input layer to hidden layer
        self.fc2 = nn.Linear(32, 16)  # Hidden layer
        self.fc3 = nn.Linear(16, 1)   # Hidden layer to output layer
        self.sigmoid = nn.Sigmoid()   # Sigmoid activation for binary classification

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # ReLU activation
        x = torch.relu(self.fc2(x))  # ReLU activation
        x = self.sigmoid(self.fc3(x))  # Sigmoid activation
        return x

# Step 5: Create the model, define the loss function and the optimizer
model = MLP()
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer

# Step 6: Train the model
epochs = 100
for epoch in range(epochs):
    model.train()
    
    # Forward pass
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# Step 7: Evaluate the model
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    y_pred = (y_pred > 0.5).float()  # Convert probabilities to class labels
    accuracy = (y_pred.eq(y_test_tensor).sum() / y_test_tensor.size(0)).item()
    print(f'Accuracy: {accuracy:.2f}')

# Step 8: Visualization of decision boundaries
def plot_decision_boundary(X, y, model):
    # Create a mesh grid for the plot
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))

    # Predict on the mesh grid points
    with torch.no_grad():
        Z = model(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()]))
        Z = (Z > 0.5).float().numpy()  # Convert probabilities to class labels
    Z = Z.reshape(xx.shape)

    # Plotting
    plt.figure(figsize=(10, 6))  # Set the figure size
    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.coolwarm)
    plt.title("MLP Decision Boundary")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

# Call the function to plot the decision boundary
plot_decision_boundary(X_test, y_test, model)
